---
layout: default
title: Introduction
permalink: introduction
---
## Introduction

<p>Annotation is one of John Unsworth’s scholarly primitives: processes essential for research, pedagogy, and interpretation. Although most often considered an affordance of print texts, it’s also useful for scholars working with digital sound recordings. In the realm of digital (digitized and/or born-digital) sound, annotation manifests in diverse forms from metadata that provides contextualizing information, to transcriptions which remediate sound, to personal timestamped notes. These annotation forms are context-specific, where context is multi-layered and can refer to, for instance, the format of the audio recording, the repository in which it is displayed, and the listening paradigm of the annotator. Because annotation is not an ahistorical, incidental consequence of interacting with audio digitally but rather a situated artifact of a given listening experience, it holds the possibility of critical reflexivity toward an audiotext and its various forms of mediation. Typical thought about annotation tends to prioritize its functions as document of “authorial intention,” (as in paratext and sometimes marginalia) or else a means of making an item “system-aware” (as in metadata schemes used in online repositories) (Clement and Fischer, par. 1, par. 3). However, recent scholarship has produced both theories (e.g. Clement and Fischer’s “audiated annotation”) and case studies (e.g. The SpokenWeb Digital Anthology, published on AVAnnotate) reframing digital annotation as a critical-interpretive practice in itself. Based on these developments, I use annotation here as a way to think reflexively about how I represent the features of a given audiotext alongside the forms of mediation that inform my listening. Annotation, on these terms, is not an objective practice of indexation, it is constitutive, a perceptual construct that produces (rather than merely representing) the audiotext. 
</p>

<p>I use <i>reflexivity</i> to denote responsibility to the relations of annotation. Reflexivity often refers to circular, recursive, or self-referential processes of (re)interrogation. As opposed to framings of annotation as stable objects of study and/or use, a reflexive approach frames annotation as a decentralized, subjective process. I consider my own role with respect to the normative social and technological paradigms of listening to sound, processing representations of sound, and writing about sound in/through which I work. I identify three areas of annotation—description, indexing, and paratext—through which I encounter both the audiotext and these norms of working with sound. Given my central thesis—that annotation re-constitutes the audiotext—this isn’t just a matter of becoming aware of how I relate to sound, and through which social and technological avenues. It’s a matter of responsibility, of deciding how I use vocabulary, timestamps, and paratext to ethically re-present the audiotext. “Understanding is a process,” writes Charles Husband, “and as such it is a catalyst that actively, even dangerously, interacts and changes whatever it comes into contact with” (qtd. in Lipari 183). Here, I ask: how do my uses of description, indexing, and paratext interact with the audiotext? What does it mean to act reflexively, through these practices, to an audiotext? And how can I mobilize these concerns toward a usable annotation methodology?</p>

<p>I’ve defined reflexivity as responsibility to the relations of annotation. I therefore begin from Lisbeth Lipari’s understanding of responsibility (through listening) as “a posture of receptivity that can receive the other without assimilation or appropriation” (197). Because this is an intersubjective approach, I must not begin with a specific method or theory into which the audiotext is subsumed. Rather, I start with an aim of “creat[ing] a dwelling space to receive the alterity of the other and let it resonate” (198). Listening to alterity is a political activity. Dylan Robinson notes the extractive tendencies of settler colonial “hungry listening,” in which the encounter with sound is not an intersubjective meeting but a hierarchical relationship in which the listener has the power to “dismiss, affirm, or appropriate sound as content” (16). This type of listening is upheld through the categorization of sound into identifiable and describable features of “formal structures, generic features, or particular musical representations and characterization” (50). Robinson explains:
	[T]his ‘listening for’ satiates through familiarity (to feel pleasure from the satisfaction of identification and recognition) but also through 	certainty (to feel pleasure from finding the ‘fit’ of content within a predetermined framework). (50-51)
The utility of “listening for” schemes is evident in annotations of audio, particularly when those annotations purport to directly describe or transcribe the audio event. With a pre-established vocabulary of terms which can be indexed to audio, annotation can reach toward the rationalized Western settler “construct of the hearing ear as something operational, quantifiable, and separable from subjective experience” (Sterne 68). Set vocabularies, units for quantification of sound, and other abstractions of sound are so enticing because they obscure the friction that characterizes the encounter with alterity. Because these ways of representing sound are the most widely intelligible to sound authorities (both human and technological) they are discursively normalized as ahistorical and objective. This is both false and prohibitive of an intersubjective encounter between listener and audio. If I don’t consider my representation of sound subjective, then my responsibility to the audiotext becomes nebulous and cursory because it cannot actually impact my annotations.
</p>

<p>Here, I emphasize subjectivity and situatedness by focusing on the friction of my encounters. I encounter sound, but I also encounter vocabularies of sound; Audacity, an audio editing app that I use for playback; and AVAnnotate, the platform I use to publish my annotations. In considering the composite of all of these encounters I follow Jerome McGann treatment of texts as “autopoietic mechanisms” which operate through “linguistic and bibliographical codings” and “cannot be separated from those who manipulate and use them” (15). While McGann’s theory is focused on print texts, I extend it into the audio realm by discussing three of the codes through which annotation re-constitutes sound: description, timestamp indexing, and paratext. Each of these codes gestures not only to my encounter with sound, but the processes by which I gain representational access to sound: I describe sound through various established vocabularies of sound; I create timestamps using Audacity; and I create paratext using AVAnnotate. When I refer to the audiotext, I refer to all of these interfaces at which (physical, vibrational) sound gains representative meaning. “The audiotext,” writes Jason Camlot in <i>Phonopoetics</i>, “is an interpretive concept by which sound is conceptualized as a signal with ideational, aesthetic, social, cultural, and formal qualities of historical significance” (11). To encounter an audiotext is to encounter each of these qualities, and to reconstitute it through description, indexing, and paratext is to reconstitute each of these qualities (even unintentionally). Reflexivity to the audiotextual interfaces of description, indexing, and paratext engenders reflexivity to the values already ingrained in these technologies: how they (purport to) create access to sound, which historically-situated paradigm of listening they engender, how they obscure (or do not obscure) the encounter with sounded alterity. I apply reflexivity by applying these considerations to my creation of descriptions, indexes, and paratext. Friction is produced in the gap between the form of representation that is the easiest, and that which is the most reflexive. This may lead to a loss of theoretical rigor in my annotation methodology, or a lack of intelligibility. In part, these losses represent an encounter with alterity that leaves alterity intact.
</p>

<p><i>The "Operator" Audiotexts</i></p>

<p>Because reflexive annotation is centered around a critical understanding of the specific features that constitute an audiotext, it is not a methodology that is arbitrarily applied to an audiotext but rather created in conversation with an audiotext. Therefore, this project is a case study in reflexive annotation. My annotations are of three distinct recordings of the same performance: Oana Avasilichioaei’s performance of “Operator” at the 2019 SpokenWeb Symposium. Although each recording is of the same performance, they come from different recording devices and sound noticeably different. For this reason, these three recordings are particularly useful for the purposes of reflexive annotation. I annotate each of the recordings, which I refer to using the arbitrary titles of Mic 1, Mic 2, and Mic 3. The annotations must be particularly attentive to the subtleties of each of the recordings—and how these subtle differences engender/encourage subtly different forms of mediated listening—to avoid collapsing the distinctions between them.</p>

<p>Using three different recordings also provides an opportunity to explore various distinct ways in which reflexive annotation can manifest, and to avoid prescribing a single methodology. Of course, having multiple recordings is not a prerequisite to reflexively annotating. Although I compare the three, my annotations of any of them are not contingent upon the other two. In other words, I don’t differentiate for the sake of differentiation.</p>

<p>This performance comes from an event entitled “The Politics and Poetics of Mediated Sound.” “Operator,” a poem from Avasilichioaei’s book <i>Eight Track</i>, is a reflection about mediation in both content and form. The poem inverts the subject position of military drone operators by casting them as the object of surveillance. In the poem, technological mediation functions to abstract, to distance the operators from the material implications of their digital actions. But, for Avasilichioaei, the abstractive forces of sound and language can also be used to defamiliarize the position of operator and to bring to light the network of technological, social, and militaristic power relations on which it relies.</p>

<p>“Operator” is sounded through vocal, technological, instrumental, and other means. Through my annotation, I re-constitute these sounds as representational signal. In this introduction to my annotations, I introduce my impression of the sounds in each of the recordings; how they are linked to my interface with vocabularies of sound, Audacity, and AVAnnotate; and how I use reflexivity to represent them through description, indexing, and paratext. I refer to the Mic 1, Mic 2, and Mic 3 annotations, linked on the homepage of this project, throughout. I close with a reflection about my annotation process and a discussion of the function of reflexive annotation.</p>

## Description
<p>The critical significance of vocabularies of annotation is not only in their affordances as means of representation, but also in how they situate sound and the listener. Vocabularies for describing sound are often selected as a result of functional need (e.g. transcripts for a podcast) or technological affordances (e.g. the parameters of a given metadata scheme). Vocabularies are also determined by the digital context in which the annotation exists, and the expectations of viewers/users. Here, the aim is ease of access, the smoothing over of the encounter between the reader (of an annotation) and the sound of an audiotext. Tools like AVAnnotate, which supports a variety of forms of annotation, allow more creative latitude for the use of vocabularies that aren’t oriented solely toward functionality. There are several established critical-analytical paradigms for sound description: prosody, which focuses on establishing metrical structure, is commonly applied to sounded poetry (Bernstein 14). Transcription, which is often used to access sound for the purpose of analysis, “demand[s] a certain degree of envoicement from the reader” and may obscure the semiotic qualities of the audiotext (Camlot and Mitchell). Beyond these and other common forms, which reconstitute audiotext through/as its easily-describable traits, there have been attempts to create vocabularies that are holistically responsive to semiotic features of sound: Pierre Schaeffer proposes a classification system that is independent of a sound’s cause in his <i>Traité des objets musicaux</i> (Chion 27), and Bernstein cites Peters and Sherman’s sixty-character “special font to document the sounds, rhythms, and melodies of the Afro-poetic tradition” (16). On the whole, though, to write about sound in a way that reconstitutes at least some of its semiotic character tends to be—for lack of a better term—a bit clunky. This friction, or lack of fit at the interface between sound and written language, is what I explore as a means toward reflexivity.</p>


<p>In this section, I discuss two main features of the descriptive vocabulary I use in this project: a focus on semiotic features and the use of general and relative language. I develop and rationalize my vocabulary based on attention to sounded alterity and interrogation of the sociocultural and technological codes I interface with.</p>


<p><i>Focus on semiotic features</i></p>

<p>In each of the recordings, the majority of what’s audible is not symbolic speech. When spoken language is audible, it’s complicated by semiotic features, whether from other layers of sound or striking voice modulation. For this reason, my annotation focuses on semiotic features of sound. This is most evident in my descriptions of speech, which use terms of volume, pitch, onset/offset, tone, and (occasionally) echo. Here I draw on Charles Bernstein’s description of a “poetic mode of listening,” in which there is “an oscillation (or temporal overlap) between the materially present sound…and the absent meaning” (18). It’s for this reason, also, that I rarely use spacial descriptors (i.e. background, foreground) and I don’t often attribute sounds to specific sources. This practice is informed, in part, by Michel Chion’s reduced listening, which centers around “inherent qualities of sounds,” (29) and which Chion places at odds with causal listening (i.e. listening for information about the source) and codal or semantic listening (i.e. listening for linguistic meaning).</p>

<p>While Chion distinguishes between these as three separate forms of listening, my annotations are closer to, in Bernstein’s description, “oscillati[ng]” between them. This is true in the sense that none of my descriptors are directly indicative of innate features of sound, but instead the necessity of essentializing certain features of it, using constructed ways of describing sound (e.g. volume and pitch), in order to be able to write about it. It’s also true in the sense that there are causal and semantic descriptions mixed into my semiotic descriptions. In annotations for each audiotext, I use timbre descriptors like “electronic,” “brassy,” “ and “crunching,” which imply causes even if they don’t state them outright. When I use more specific images, I make them similes, as in “mid-pitch clicking sound, like ball bearings colliding” (Mic 2). This is to clearly frame these descriptions as conjecture.
</p>

<p>Because Mic 2 has the clearest speech, it has more descriptors of symbolic features of speech, as in the following example:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:03:13</td>
    <td>0:03:44</td> 
    <td>One mid pitch voice speaking. Little intonation (i.e. most syllables are stressed), resulting in choppy-sounding words (e.g. ve-hi-cules). Short pauses between words. Call-and-response form (e.g. "...right? Yeah, that's what it looks like to me.") suggests two speakers, but there's only one voice. Terms that presumably refer to descriptive rather than spoken text (e.g. "unintelligible," "explicative") included within flow of speech, uttered by same speaker.</td>
  </tr>
</table>

<p>In this example, I discuss a symbolic “call and response form” which is contradicted by semiotic features denoting a single voice. Additionally, I isolate words which I assume to “refer to descriptive rather than spoken text,” implicating their disruption of the syntactic structures of the speech, and evoking the forms of audio transcription which use descriptors like <i>[unintelligible]</i>. In this case, including these descriptions highlights how the audiotext draws attention to the blurry line between the semiotic and the semantic: does “unintelligible” function the same way as other symbolic words in this context, or is it closer to an index? When questions like this arise from the audiotext, they are obscured by a sole focus on semiotic features of speech. I alter my descriptions, when needed, to fit these complexities of the audiotext. 
</p>

<p>Bernstein’s discussion of “absent meaning” is particularly relevant to the Mic 3 audiotext, in which spoken words are frequently inaudible (in the sense that they can’t be made out as symbolic language). To seek the symbolic in these sounds feels like reaching for something that’s not present among a plethora of “materially present sound[s],” more than are in either of the other audiotexts. For this reason, I’ve foregone the descriptor “Oana’s voice” in my annotations of this audiotext, not because her voice is unrecognizable, but because this descriptor is causative (i.e. sound is attributed to voice, which is ultimately attributed to a causative speaker) and privileges the voice relative to other sounds, when it does not function in a privileged manner in this audiotext itself, but rather is mixed into the sonic din. Because Oana’s voice is separated by volume, pitch, and clarity from the other sounds in the Mic 1 and Mic 2 audiotexts, I use the “Oana’s voice” descriptor to indicate this. However, I maintain my focus on semiotic features of sound across all of my annotations in recognition of the skillful manner in which she manipulates her voice, imbuing it with semiotic meaning.
</p>

<p>Of course, the semiotic elements of a voiced performance are not predicated on authorial intent. What I’m suggesting here is, instead, that this audiotext is in many ways about the relationship between the symbolic and the semiotic; at what point does whispered offset become simply a whisper in itself, or when (if ever) does a repeated word lose its meaning? The “Operator” recordings interrogate the constructed boundaries between the semiotic and the semantic in complex, intermingling ways, which I can only begin to approach in my written annotations.</p>


<p><i>General and relative language</i></p>

<p>The difficulty of describing literary sound has been noted by Bernstein, who writes that “our technical vocabulary strains at accounting for more than a small portion of the acoustic activity of the sounded poem” (15). The “Operator” recordings, on the other hand, are open to unauthorized sound—because of the polyvocal, variable performance style, there is no clear delimitation, particularly in Mic 3 and Mic 1, between what is “of the performance” and what is incidental sound; the latter becomes part of each audiotext, according to the specific and variable situation of that audiotext in space and media. They are, in Bernstein’s description of the sounded poem, “always at the edge of semantic excess,” (13) open and inviting to what lies beyond. In recognition of the impossibility of putting the extra-semantic into language, I’ve selected descriptors that are relative and evidently subjective. Relative descriptors indicate the extent (“low,” “mid,” “high,” “soft,” “hard”) of qualities including pitch, volume, and onset/offset. I also variously use other relative descriptors (e.g. “sporadic”). This entails a loss of rigor—someone seeking to, for instance, digitally reconstruct the sounds solely from my annotations would find them wildly insufficient—but it opens up my annotations to multiple sonic possibilities, following the model of Oana’s performance.</p>

<p>Relative language is also reflective of the process I used to determine elements like pitch and volume. Using Audacity, I created waveforms and spectrograms for each audiotext. This is a multi-view of the waveform (top) and spectrogram (bottom) for each of the channels in the Mic 3 recording:</p>

<img width="908" alt="Screenshot of Audacity interface, including waveform and spectogram correcponding to Mic 3." src="https://github.com/sarah-freeman/operator-annotations/assets/142846974/3fb9dfbc-b8bf-4a36-b0d6-0ce90f269a9d">

<p>I based my descriptions on my perception of the audio and, secondarily, what was represented in the images. If I describe a sound as high volume, for instance, it’s because it sounded loud relative to the surrounding sounds, but also because there was likely a peak on the waveform at that point. When I struggled to discern features of the sound, I relied more heavily on the waveform and spectrogram. I don’t use numerical units like decibels—although they are measured by Audacity—because those units aren’t reflective of the way in which I used the technology, which wasn’t to isolate “absolute” features of the sound at a given moment in time, but to compare the sound across time.</p>

<p>Furthermore, using precise units presumes precision that I didn’t have in my listening experience; for instance, I used Bluetooth headphones, which likely compressed the sound as I heard it, so what I saw on Audacity—even by the numbers—is not necessarily representative of what I heard.</p>

<p>One benefit of these descriptors is that I can use the same terms to describe a variety of sounds; unlike technical vocabularies, they are general enough to be flexible. Another is that, because they’re relative, making sense of any given annotation requires engagement with the surrounding annotations and perhaps also the audiotext itself. For instance, a high volume sound in the Mic 2 recording is louder than the Mic 1 recording, because the Mic 2 recording is generally louder. This both centers polyphony and displaces meaning from any single location, written or sounded. This, I would argue, is a descriptive practice which, while lacking in rigor and not suited for all purposes, is consistent with the polyphony, fluidity, and openness of the audiotexts themselves. </p>

## Indexing

<p>In this section, I extend my discussion of reflexivity to the time-based indexing practices of excerpting and timestamping. Here, I am reflexive in my consideration of the contingency of accessing sound through control of its temporal situation. Unconstrained and on-demand access to sound is based on its status as a digital media object that can be saved, played, paused, rewinded, and excerpted. These actions are enabled by technology; I discuss my use of Audacity here, but tech functions in other ways to encode sound as data, to package that data in a transferable and sustainable format, and to playback that data. Sometimes, the effects of mediation are present in the audio itself; for instance, distortions in a wax cylinder can lead to cracks and humming in the audio. Wolfgang Ernst calls this “media-archeological information (about the physically real event)” (qtd. in Camlot 2). I approach indexing in a similar way: it is information not about the event of the sound, but rather my own listening event (and the subsequent creation of my annotations). I introduce time-based indexing in my discussion of excerpting, and further apply it to my timestamps, which are layered and variable-length.</p>

<p><i>Excerpting</i></p>

<p>My annotations span an excerpt of 00:06:35 from the full performance, which included several performers and lasted roughly an hour and a half. Avasilichioaei’s original performance was about 16 minutes; the excerpted portion represents the end of her performance (it cuts off right before the closing applause). I chose this portion because it is generally representative of the varieties of voices and sounds present throughout the performance. It is long enough to give a sense of the performance, and short enough to allow me to provide detailed annotations.</p>

<p>My use of this excerpt, and the ways in which I approach it, are significant to the situation of each of the recordings in relation to each other and in relation to the annotations. My excerpted section occurs at different start and end points in each of the full-length recordings, owing to different start times. Rather than reflect these times in my timestamps, I timestamp based on the excerpted duration, from 0:00:00 to 00:06:35, for all three recordings. This reflects the remediation of the recordings and facilitates comparison between the annotations.</p>

<p>To begin timestamping at 0:00:00 is to acknowledge a temporality (and set of temporal contingencies) that is distinct to the excerpts and the context in which I place them. This is distinct from the temporality of the live performance, which Friedrich Kittler calls “real time,” (5) and cannot be represented as a duration. To represent a time period as a duration entails a set of technologically mediated, time-based practices like pausing, clipping, and rewinding, which Kittler refers to as “time axis manipulation” (3). Both the original audio clips and my excerpted versions engender not only the possibility of, but the active use of time axis manipulation. The presence of an original and an excerpt implies the use of some technological means of cutting and then saving an excerpted form, which by this process of time axis manipulation is not merely a derivative of the original but a new audiotext in its own right. Thus the excerpt is both predicated on and subject to time axis manipulation in this project. </p>

<p><i>Layered timestamps and indexes</i></p>
<p>At most instances in any of the recordings, there are multiple types of sound audible. These range from an electronic bass tone to voices to static distortion. They become audible and inaudible at different times, and change at varying intervals. In these ways, they exist in time independently from each other. For this reason, I’ve timestamped them independently. This creates a non-chronological temporality in my timestamps, in that a sound cannot be said to have completely occurred before the next indexed sound, only to have begun before it. Consider, for instance, this section from Mic 1:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:41</td>
    <td>0:02:92</td> 
    <td>Sporadic mid-volume rolling sound, like a pool ball rolling across a table</td>
  </tr>
  <tr>
    <td>0:01:54</td>
    <td></td> 
    <td>Quiet breathy/blowing sound</td>
  </tr>
  <tr>
    <td>0:01:59</td>
    <td>0:02:00</td> 
    <td>Loud tap, like two pool balls colliding</td>
  </tr>
</table>

<p>One moment in time, such as 0:01:50, corresponds to multiple indices and thus has multiple layers, none of which—in the Mic 1 audiotext—are subordinate to each other even when they are infrequent/sporadic (as in the first entry above), quiet (as in the second) or short (as in the third). Mic 3 has similar interactions between layers of sounds, although they blend together a bit more, leading to more annotations that group sounds together.</p>

<p>Mic 2 is also polyphonic, but it has clearer audio quality and consequently clearly defined layers of sound. Because the layers of sound are more distinct, I am able to identify layers not just based on time (i.e. through simultaneous timestamps) but also by shared sonic characteristics between sounds at different points in the audiotext. These shared qualities inform my use of indexes to denote four general categories of sound in the Mic 2 audiotext: speaking voices, electronic tones and distortions, and friction sounds. The latter two are represented in this sample:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:36</td>
    <td>0:02:04</td> 
    <td>Gravelley, crunching sound beginning with a period of ~0.5 second. Low to mid pitch and volume. Period becomes longer and volume decreases through this interval.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:01:41</td>
    <td>0:02:02</td> 
    <td>Clear scratch sound with varying pitch and generally low volume, like a wooden stick tracing across a wooden table.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:02:10</td>
    <td>0:02:12</td> 
    <td>Polyphonic electronic tone crescendos to high volume</td>
    <td>Electronic tones and distortions</td>
  </tr>
</table>

<p>By attending to the polyphonic nature of each of these audiotexts, and the distinct manner in which sounds stratify, blend, and remediate each other, I attempt to reflect the variances in sonic layering in each.</p>

<p><i>Variable-length timestamps</i></p>

<p>To the extent that my annotations are also a visual (i.e. written text) interpretation of sound, I’ve used Audacity to preserve what may now be called “visual noise.” All of these audiotexts are noisy, in the sense that they perpetuate uncontrolled, unauthorized, and (in the case of Mic 1 and Mic 3) spatially resonant sound. Sometimes, I group these sounds together in a single annotation, as in the following from Mic 3:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:00:07</td>
    <td>0:00:08</td> 
    <td>Series of friction sounds (pops, cracks, gravelley sounds) in quick succession, at mid volume and mid-high pitch</td>
  </tr>
</table>

<p>Other times, I timestamp them individually, as in this annotation from Mic 1:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:02</td>
    <td> </td> 
    <td>Pop with hard onset and soft offset</td>
  </tr>
</table>

<p>As above, all of the annotations under one second have only an instantaneous timestamp, no end times. A timestamp under one second is a bit ridiculous in terms of what’s useful or typical, but it’s helpful in making evident my use of Audacity, and in its atypicality also indicating the subjectivity of using audio technology to gain precise access to “noisy” signals, like underlying static (as in the first example) and acute pops (as in the second), often to get rid of them.</p>

<p>Somewhat unexpectedly, there are more “visual noise” timestamps in my Mic 1 annotations than either of the other annotations, although in visual representations, it appears to have less noise than Mic 3. This is because the noise was more consistent in terms of both presence and sonic characteristics in the Mic 3 audio, and therefore I could use longer timestamps for these annotations. In Mic 1, however, it was mostly short pops and cracks which weren’t consistent with the surrounding sounds, and for that reason I annotated them separately. Mic 2 has a mix of both.</p>

## Paratext

<p>In this section, I discuss paratext of annotation. I use Genette’s definition of paratext as that which constitutes the “threshold” of a text, which through its framing of the text, also acts to lead readers to “a better reception of the text and a more pertinent reading—more pertinent, naturally, in the eyes of the author” (261, 262). In this essay, itself a paratext of my annotations, I have attempted to frame reflexive annotation as a process of responsible creation of artifacts of a situated listening experience. This is the framing I hope to reassert in all of the paratext of this project (or at least the pieces of paratext that I can control). </p>

<p>I begin by asking what and where the paratext is in this project. Differentiating text from paratext is not a trivial task; Genette writes, “The ways and means of the paratext are modified unceasingly according to periods, cultures, genres, authors, works, [and] editions of the same work” (262). Audio annotation itself can be a paratext in, for instance, digital collections of archival audio, where indexed annotations serve as a table of contents to the main (audio)text (see collections in the University of Alberta’s Aviary repository). This frames annotation as a usable derivative of the main audiotext, to be received without critical consideration of its contents. </p>

<p>Because reflexive annotation is based on framing annotation as subjective, interpretive, and singular, it’s imperative that the host platform have affordances to this end. Therefore, I open this section by discussing AVAnnotate, the platform I used to create this website. I then discuss the formatting of the annotations relative to the audio, and the placement of the three annotation pages relative to each other.</p>

<p><i>AVAnnotate</i></p>

<p>AVAnnotate formats user-created annotations of audio and/or video, and generates GitHub Pages websites to host these annotations, along with contextual writing and/or images. Because each AVAnnotate project has its own, independently-accessible website, the AVAnnotate website and branding belongs to a subcategory of paratext that Genette calls <i>epitext</i>. Epitext is material that is closely associated with the text, but outside the space of the volume (the book, or in this case this website) itself (264). Although this website can be accessed independently from the AVAnnotate website, the platform provides much of the framing for this project. AVAnnotate is oriented primarily towards researchers and scholars who want to create (not just view) annotations. Framing annotation as an end in itself highlights the critical-analytical and/or reflective possibilities engendered in the act of creating annotations. 
Consequently, AVAnnotate allows users lots of latitude in determining layout, appearance, and contextualizing information; while the AVAnnotate platform itself is used to upload indexed annotations, the project website can be edited in any number of ways via its linked GitHub repository. It’s a tool that doesn’t presume an end function, and as such it reorients (creative and receptive) focus to the process of creating and contextualizing annotations.</p>

<p><i>Formatting of annotation pages</i></p>

<p>Following from the affordances of AVAnnotate, I utilize <i>peritext</i>—paratext inside this website—to highlight the independence of my annotations from the audio itself (Genette 263). Peritext manifests in many ways; for instance, the title of this website, and the descriptions in the footer and at the beginning of the pages are all examples of peritext. In these instances, many of my choices are self-evident. For instance, I place the term “reflexive annotation” early in my title and the footer description to emphasize that this is a project about critical uses of annotation, not merely a host for audio. To avoid repeating the self-evident, I do not discuss every instance of peritext here but focus on the formatting of the annotation pages, via the viewer selection.</p>

<p>AVAnnotate supports two IIIF-compatible viewers: the Aviary Player and the Universal Viewer. The Aviary Player format integrates the video/audio playback and the annotations into a single view, with the playback occupying the left and center two-thirds of the view, and the annotations at the right third of the screen. The Universal Viewer has more separation between the playback and the annotations, where the playback is at the top of the screen and the annotations follow. I use the Universal Viewer because it’s more practical for an audio-only project, but also because it visually separates the audio from the annotations. This separation supports the principle of responsibility to sounded alterity that I’ve used throughout this project, because it highlights that my annotations do not enter into the space or time of the audio itself, and rather serves as an exterior artifact of a mediated listening experience.</p>

## Conclusion

<p>This project introduction explores how the central axioms of an audio annotation process can be determined not by usage conventions, but instead by reflexivity in the annotation process. I define reflexively as responsibility to the network of relations that (re)constitute the audiotext. This encompasses the location of sociopolitical values and aims within my own listening experience and respect for the alterity of the audiotext.
</p>

<p>I apply reflexivity in three areas of my annotations: description, indexing, and paratext. I do this by tracing the encounters I engage in, with the sound of the audiotext but also with vocabulary schemes for describing sound, with Audacity, and with AVAnnotate. The resulting annotations utilize relative language that highlights semiotic and polyphonic features of the audiotexts; layered timestamps that reflect the temporal mediation of excerpting and inscribing non-chronological sonic events; and paratext that foregrounds the non-derivative, situated subjectivity of each annotation set. My annotations are not primarily about the sounds I listened to; they’re a document of the processes I undertook to attempt to inscribe those sounds in the most reflexive way possible. Ultimately, and perhaps paradoxically, I found that to annotate reflexively means to give up the goal of accessing sound through written language. I found myself more drawn to the question of why even annotate? What can annotation do when it isn’t taken for granted? Reflexive annotation gives me three answers to these questions.</p>

<p>	<b>1. Reflexive annotation provides a (preliminary) framework for critically and responsibly working with literary audio.</b>I began this essay with 	a discussion of Unsworth’s scholarly primitives, which are the guiding processes by which knowledge creation is approached in institutional, academic 	settings. These can be taken for granted with respect to print texts; I don’t remember any particular moment when I decided how I would write 		marginal notes in printed books, or which criteria I would use to pick out quotes from a text. I rarely begin with a methodology in mind, even 		through my intuitive practices are just as contingent and situated in specific sociopolitical and epistemological aims as the audio-related practices 	I’ve discussed here. Working with audio is a less intuitive practice not because of some inherent incompatibility, but because of the friction 		between the print-related practices I know and the reality that digital audio is a completely different form of media.</p>
	
<p>	Attempting to work through this gap, while frequently frustrating, is also the locus for a methodological approach to sound that is attuned to its 	own status as methodology. To meet that friction, and to consider what it sounds like and where it comes from, is an interesting and (as I’ve tried 	to demonstrate in this essay) generative prospect. From this lens, I can build a framework for writing about, indexing, and publishing work about 	audio that is less constrained by the norms of print.</p>

<p>	<b>2. Reflexive annotation documents a listening practice.</b> Annotation doesn’t preserve the sensory experience of listening, but it does preserve 	the representational choices produced by/within a specific way of listening. These choices can then be analyzed, reconsidered, and responded to. One 	audiotext can be annotated in multiple different ways, and the ways in which it is reconstituted can be compared. Through reflexive annotation, 	listening is not a ephemeral, passive process of obtaining information but itself a product of its own situation, which is reproduced in annotation </p>

<p>	<b>3. Reflexive annotation is decentralized.</b> It takes into consideration both processes that aren’t visible in the annotations (e.g. my use of 	Audacity) and elements that are visible, but outside of the space of the annotation itself (e.g. paratexts via AVAnnotate). Because reflexive 		annotation is decentralized, it brings the technologies I use under the umbrella of critical consideration. Both my process and the resulting 		annotations would have varied greatly if I used different tools, particularly in the case of AVAnnotate. As far as I’m aware, there isn’t another 	platform that is oriented specifically towards the creation of annotations for digital audio. It would have been much more difficult to create this 	project without the use of AVAnnotate. Because reflexive annotation is linked to the affordances of the platform(s) it uses, it highlights the 		importance of platforms—like AVAnnotate—that allow latitude for practical applications of reflexivity.</p>


## Works Cited

<p>Bernstein, Charles. “Introduction.” Close Listening: Poetry and the Performed Word, Oxford University Press, New York, NY, 1998, pp. 3–26.</p>

<p>Camlot, Jason, and Christine Mitchell. “Amodern 4: The Poetry Series.” Amodern, vol. 4, Mar. 2015, https://doi.org/https://amodern.net/issues/amodern-4/. 
Camlot, Jason. Phonopoetics: The Making of Early Literary Recordings. Stanford University Press, 2019.</p>

<p>Chion, Michel. “The Three Listening Modes.” Audio-Vision: Sound on Screen, translated by Claudia Gorbman, Second ed., Columbia University Press, New York, NY, 2019, pp. 22–34.</p>

<p>Clement, Tanya E., and Liz Fischer. “Audiated Annotation from the Middle Ages to the Open Web.” Digital Humanities Quarterly, vol. 15, no. 1, 2021, http://www.digitalhumanities.org/dhq/vol/15/1/000512/000512.html.</p>

<p>​​“Collections.” Aviary, University of Alberta, ualberta.aviaryplatform.com/collection. Accessed 15 Apr. 2024. </p>

<p>Genette, Gérard. “Introduction to the Paratext.” Translated by Marie Maclean. New Literary History, vol. 22, no. 2, spring 1991, pp. 261–272, https://doi.org/10.2307/469037.</p>

<p>Kittler, Friedrich. “Real Time Analysis, Time Axis Manipulation.” Translated by Geoffrey Winthrop-Young. Cultural Politics, vol. 13, no. 1, 2017, pp. 1–18, https://doi.org/10.1215/17432197-3755144.</p>

<p>Lipari, Lisbeth. Listening, Thinking, Being: Toward an Ethics of Attunement. The Pennsylvania State University Press, 2014.
McGann, Jerome J. The Textual Condition. Princeton University Press, 1991.</p>

<p>Robinson, Dylan. Hungry Listening: Resonant Theory for Indigenous Sound Studies. University of Minnesota Press, 2021.</p>

<p>Sterne, Jonathan. “Hearing.” Keywords in Sound, Duke University Press, 2015, pp. 65–77.</p>

<p>Unsworth, John. “Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This?” John M. Unsworth: Conference Papers and Presentations, University of Virginia, 13 May 2000, people.brandeis.edu/~unsworth/Kings.5-00/primitives.html.</p>
