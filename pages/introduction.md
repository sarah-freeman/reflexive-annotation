---
layout: default
title: Introduction
permalink: introduction
---
## Introduction

<p>Annotation is one of John Unsworth’s scholarly primitives, processes that form the basis of academic knowledge production across disciplines. Although primarily considered an affordance of print texts, it’s also useful for work with digital sound recordings. In the realm of digital (digitized and/or born-digital) sound, annotation manifests in diverse forms from metadata that provides contextualizing information, to transcriptions which remediate sound, to personal timestamped notes. Typically, annotation is theorized as paratext, marginalia, and/or metadata, the first two terms prioritizing its function as document of “authorial intention,” and the latter as a means of making an item “system-aware” (Clement and Fischer, par. 1, par. 3). However, recent scholarship has produced both theories and case studies reframin annotation as a critical-interpretive practice in itself. For instance, Clement and Fischer’s “audiated annotation” highlights the critical significance of “unauthorized, distributed, and decentralized” annotations as a means of accessing audio when the sound itself is unavailable (par. 4). Because annotation is not an ahistorical, incidental consequence of interacting with audio digitally but rather an artifact of a given listening situation, it holds the possibility of critical reflexivity toward an audiotext and its various forms of mediation. Viewed this way, annotation moves beyond an objective or merely representative practice of indexation. It is constitutive, a perceptual construct that itself produces the audiotext.
</p>

<p>I use <i>reflexivity</i> to denote responsibility to the series of encounters that makes up the annotation process. Reflexivity often refers to circular, recursive, or self-referential processes of (re)interrogation. As opposed to framings of annotation as stable objects of study and/or use, a reflexive approach frames annotation as a decentralized, subjective process. I consider my own role as an annotator with respect to the normative social and technological paradigms of listening to sound, processing representations of sound, and writing about sound in/through which I work. I identify three areas of annotation—description, indexing, and paratext—through which I encounter both the audiotext and these norms of working with sound. Given my central thesis—that annotation reconstitutes the audiotext—this isn’t just a matter of becoming aware of how I relate to sound, and through which social and technological avenues. It’s a matter of responsibility, of deciding how I use vocabulary, timestamps, and paratext to ethically reconstitute the audiotext. “Understanding is a process,” writes Charles Husband, “and as such it is a catalyst that actively, even dangerously, interacts and changes whatever it comes into contact with” (qtd. in Lipari 183). Here, I ask: how does my use of description, indexing, and paratext interact with the audiotext? What does it mean to act reflexively, through these practices? And how can I mobilize these concerns toward a usable audio annotation methodology?</p>

<p>I’ve defined reflexivity as responsibility to the relations of annotation. I therefore begin from Lisbeth Lipari’s understanding of responsibility (through listening) as “a posture of receptivity that can receive the other without assimilation or appropriation” (197). Because this is an intersubjective approach, I must not begin with a specific method or theory into which the audiotext is subsumed. Rather, I start with an aim of “creat[ing] a dwelling space to receive the alterity of the other and let it resonate” (198). Listening to alterity is a political activity. Dylan Robinson notes the extractive tendencies of settler colonial “hungry listening,” in which the encounter with sound is not an intersubjective meeting but a hierarchical relationship in which the listener has the power to “dismiss, affirm, or appropriate sound as content” (16). This type of listening is upheld through the categorization of sound into identifiable and describable features of “formal structures, generic features, or particular musical representations and characterization” (50). Robinson explains:
	[T]his ‘listening for’ satiates through familiarity (to feel pleasure from the satisfaction of 	           identification and recognition) but also through certainty (to feel pleasure from finding the    	‘fit’ of content within a predetermined framework). (50-51)
The utility of “listening for” schemes is evident in annotations of audio, particularly when those annotations purport to directly describe or transcribe the audio event. With a pre-established vocabulary of terms which can be indexed to audio, annotation can reach toward the rationalized Western settler “construct of the hearing ear as something operational, quantifiable, and separable from subjective experience” (Sterne 68). Set vocabularies, units for quantification of sound, and other abstractions of sound are so enticing because they obscure the friction that characterizes the encounter with alterity. Because these ways of representing sound are the most widely intelligible to sound authorities (both human and technological) they are discursively normalized as ahistorical and objective. This is both false and prohibitive of an intersubjective encounter between listener and audio. If I don’t consider my representation of sound subjective, then my responsibility to the audiotext becomes nebulous and cursory because it cannot actually impact my annotations.
</p>

<p>Here, I emphasize subjectivity and situatedness by focusing on the friction of my encounters. I encounter sound, but I also encounter vocabularies of sound; Audacity, an audio editing app that I use for playback; and AVAnnotate, the web platform I use to publish my annotations. In considering the composite of all of these encounters I follow Jerome McGann treatment of texts as “autopoietic mechanisms” which operate through “linguistic and bibliographical codings” and “cannot be separated from those who manipulate and use them” (15). While McGann’s theory is focused on print texts, I extend it into the audio realm by discussing three of the codes through which annotation reconstitutes sound: description, indexing, and paratext. Each of these codes gestures not only to my encounter with sound, but the processes by which I gain representational access to sound: I describe sound through various established vocabularies of sound; I create timestamps using Audacity; and I create paratext using AVAnnotate. When I refer to the audiotext, I refer to all of these interfaces at which (physical, vibrational) sound gains representative meaning. “The audiotext,” writes Jason Camlot in <i>Phonopoetics</i>, “is an interpretive concept by which sound is conceptualized as a signal with ideational, aesthetic, social, cultural, and formal qualities of historical significance” (11). To encounter an audiotext is to encounter each of these qualities, and to reconstitute it through description, indexing, and paratext is to reconstitute each of these qualities (even unintentionally) from a certain angle. Reflexivity to the audiotextual interfaces of description, indexing, and paratext engenders reflexivity to the values already ingrained in these technologies: how they (purport to) create access to sound, which historically-situated paradigm of listening they encode, how they obscure (or do not obscure) the encounter with sounded alterity. Friction is produced in the gap between the easiest encounter with a given interface,
and that which is the most reflexive. This may lead to a loss of theoretical rigor in my annotation methodology, or a lack of intelligibility. In part, these losses represent an encounter with alterity that leaves alterity intact.
</p>

<p><i>The "Operator" Audiotexts</i></p>

<p>Because reflexive annotation is centered around a critical understanding of the specificities that make up an audiotext, it is not a methodology that is arbitrarily applied but rather created in conversation with an audiotext. Therefore, this project is a case study in reflexive annotation. My annotations are of three distinct recordings of the same performance: Oana Avasilichioaei’s performance of “Operator” at the 2019 SpokenWeb Symposium. The recordings come from different recording devices and sound noticeably different. For this reason, these three recordings are particularly useful for the purposes of reflexive annotation. I annotate each of the recordings, which I refer to using the arbitrary titles of Mic 1, Mic 2, and Mic 3. The annotations must be particularly attentive to the subtleties of each of the recordings—and how these subtle differences engender/encourage subtly different forms of mediated listening—to avoid collapsing the distinctions between them.</p>

<p>Using three different recordings also provides an opportunity to explore various distinct ways in which reflexive annotation can manifest, and to avoid prescribing a single methodology. Of course, having multiple recordings is not a prerequisite to reflexively annotating. Although I compare the three, my annotations of any of them are not contingent upon the other two. In other words, I don’t differentiate for the sake of differentiation.</p>

<p>The performance in the audiotexts comes from an event entitled The Politics and Poetics of Mediated Sound. “Operator,” a poem published in Avasilichioaei’s book <i>Eight Track</i>, is a reflection about mediation in both content and form. The poem inverts the subject position of military drone operators by casting them as the object of surveillance. In the poem, technological mediation functions to abstract, to distance the operators from the material implications of their digital actions. But the abstractive forces of sound and language are also used to defamiliarize the position of operator, bringing to light its technological, social, and militaristic contingencies.
</p>

<p>“Operator” is sounded through vocal, technological, instrumental, and other means. Through my annotation, I reconstitute these sounds as representational signal. In this introduction to my annotations, I introduce my impression of the sounds in each of the recordings; how they are linked to my interface with vocabularies of sound, Audacity, and AVAnnotate; and how I use reflexivity to represent them through description, indexing, and paratext. I refer to the Mic 1, Mic 2, and Mic 3 annotations, linked on the homepage of this project, throughout. I close with a reflection about my annotation process and a discussion of the function of reflexive annotation.</p>

## Description
<p>The critical significance of vocabularies of annotation is not only in their affordances as means of representation, but also in how they situate sound and the listener. Vocabularies for describing sound are often tied to functional need (e.g. transcripts for a podcast) or technological affordances (e.g. the parameters of a given metadata scheme). Vocabularies are also determined by the digital context in which the annotation exists, and the functional expectations of viewers/users. Here, the aim is ease of access, the smoothing over of the encounter between the reader (of an annotation) and the sound of an audiotext. Beyond vocabularies that are oriented solely toward functionality, there are several established ways for talking about sound in critical-analytical ways. One of the most common is prosody, which focuses on establishing metrical patterns in (most often) sounded poetry (Bernstein 14). Another form is transcription, which focuses on accessing sounded language for the purpose of analysis. Transcription “demand[s] a certain degree of envoicement from the reader” and may obscure the non-linguistic semiotic qualities of the audiotext (Camlot and Mitchell). Beyond these and other common forms, which reconstitute audiotext through/as its easily-describable traits, there have been attempts to create vocabularies that are holistically responsive to the semiotics of sound: Pierre Schaeffer proposes a classification system that describes sound independent of cause in his <i>Traité des objets musicaux</i> (Chion 27), and Bernstein cites Peters and Sherman’s sixty-character “special font to document the sounds, rhythms, and melodies of the Afro-poetic tradition” (16). On the whole, though, to write about sound in a way that centers its semiotic character tends to be—for lack of a better term—a bit clunky. This friction, or lack of fit at the interface between sound and written language, is what I explore as a means toward reflexivity.</p>


<p>I base my descriptive vocabulary on characteristics of the audiotexts, focusing especially on elements that most descriptive schemes (and accompanying ways of listening) do not address. This I do in recognition that I myself am situated in this history of language use, and without reflexivity I am also drawn to ease of use. Instead, I use my vocabulary to draw attention to the network of relations that produces sound through written language. This extends from the sound of the audiotexts into technology and theories of sound and language. I demonstrate how these factors intertwine through two main features of my descriptive vocabulary: a focus on semiotic features and the use of general and relative language.</p>


<p><i>Focus on semiotic features</i></p>

<p>In each of the recordings, the focus is not on the clear conveyance of language through speech. When spoken language is audible, it’s complicated by semiotic features, whether from other layers of sound or striking vocal modulation. For this reason, my annotation focuses on semiotic features of sound. This is most evident in my descriptions of speech, which use terms of volume, pitch, onset/offset, tone, and (occasionally) echo. I do not transcribe language, except to illustrate one of these features. Here I draw on Charles Bernstein’s description of a “poetic mode of listening,” in which there is “an oscillation (or temporal overlap) between the materially present sound…and the absent meaning” (18). It’s for this reason, also, that I rarely use spacial descriptors (i.e. background, foreground) and I don’t often attribute sounds to specific sources. This practice is informed, in part, by Michel Chion’s reduced listening, which centers around “inherent qualities of sounds,” (29) and which Chion places at odds with causal listening (i.e. listening for information about the source) and codal or semantic listening (i.e. listening for linguistic meaning).</p>

<p>While Chion distinguishes between these as three separate forms of listening, my annotations are closer to, in Bernstein’s description, “oscillati[ng]” between them. This is true in the sense that none of my descriptors are directly indicative of innate features of sound, but instead the necessity of essentializing certain features of it, using constructed ways of describing sound (e.g. volume and pitch), in order to be able to write about it. It’s also true in the sense that there are causal and semantic descriptions mixed into my semiotic descriptions. In annotations for each audiotext, I use timbre descriptors like “electronic,” “brassy,” “ and “crunching,” which imply causes even if they don’t state them outright. When I use more specific images, I make them similes, as in “mid-pitch clicking sound, like ball bearings colliding” (Mic 2). This is to clearly frame these descriptions as conjecture, emphasizing that I do not access the live event of the performance.</p>

<p>Because Mic 2 has the clearest speech and thus the clearest presence of semantic language, I use a few more semantic descriptors, as in the following example:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:03:13</td>
    <td>0:03:44</td> 
    <td>One mid pitch voice speaking. Little intonation (i.e. most syllables are stressed), resulting in choppy-sounding words (e.g. ve-hi-cules). Short pauses between words. Call-and-response form (e.g. "...right? Yeah, that's what it looks like to me.") suggests two speakers, but there's only one voice. Terms that presumably refer to descriptive rather than spoken text (e.g. "unintelligible," "explicative") included within flow of speech, uttered by same speaker.</td>
  </tr>
</table>

<p>In this example, I discuss a “call and response form” which is contradicted by the sound of only a single voice. Additionally, I isolate words which I assume to “refer to descriptive rather than spoken text,” implicating their disruption of typical syntactic structures, and evoking the forms of audio transcription which use descriptors like <i>[unintelligible]</i>. In this case, including these descriptions highlights how the audiotext draws attention to the blurry line between the semiotic and the semantic: does “unintelligible” function the same way as other symbolic words in this context, or is it closer to an index? When questions like this arise from the audiotext, they are obscured by a sole focus on semiotic features of speech. I alter my descriptions, when needed, to fit these complexities of the audiotext.

</p>

<p>Another complexity of these audiotexts is the role of voice relative to other forms of sound. Voice is privileged in Mic 1 and Mic 2 by higher volume, more clarity, and longer stretches in duration. In these audiotexts, voicings by Avasilichioaei have a distinctly authorial feel relative to other sounds. I use the descriptor “Avasilichioaei’s voice” in my annotations of these audiotexts. In the Mic 3 audiotext, where voice blends into the sonic din and spoken words are frequently inaudible (in the sense that they can’t be made out as symbolic language), I use “vocalizations” instead. This isn’t because Avasilichioaei’s voice is unrecognizable; it’s in recognition of the legitimation afforded when a piece of audio is labelled with the author’s name. Both “Avasilichioaei’s voice” and “vocalizations” are true to the audio; I select the former for Mic 1 and Mic 2 and the latter for Mic 3 in recognition that how sound is described reconstitutes its relative significance.</p>


<p><i>General and relative language</i></p>

<p>The difficulty of describing literary sound has been noted by Bernstein, who writes that “our technical vocabulary strains at accounting for more than a small portion of the acoustic activity of the sounded poem” (15). Beyond the authorized, monovocal elements that typical vocabularies focus on, the “Operator” recordings are open to unauthorized sound. Because of Avasilichioaei’s polyvocal, variable performance style, there is no clear delimitation—particularly in Mic 3 and Mic 1—between what is “of the performance” and what is incidental sound. The latter becomes part of each audiotext, according to the specific and variable situation of that audiotext in space and media. They are, in Bernstein’s description of the sounded poem, “always at the edge of semantic excess,” (13) open and inviting to what lies beyond. In recognition of the impossibility of putting the extra-semantic into language, I’ve selected descriptors that are relative and evidently subjective. Relative descriptors indicate the extent (“low,” “mid,” “high,” “soft,” “hard”) of qualities including pitch, volume, and onset/offset. I also variously use other relative descriptors (e.g. “sporadic”). This entails a loss of rigor—someone seeking to, for instance, digitally reconstruct the sounds solely from my annotations would find them wildly insufficient—but it opens up my annotations to multiple sonic possibilities, following the model of  Avasilichioaei’s performance.</p>

<p>Relative language is also reflective of the process I used to determine elements like pitch and volume. Using Audacity, I created waveforms and spectrograms for each audiotext. This is a multi-view of the waveform (top) and spectrogram (bottom) for each of the channels in the Mic 3 recording:</p>

<img width="908" alt="Screenshot of Audacity interface, including waveform and spectogram correcponding to Mic 3." src="https://github.com/sarah-freeman/operator-annotations/assets/142846974/3fb9dfbc-b8bf-4a36-b0d6-0ce90f269a9d">

<p>I based my descriptions on my perception of the audio and, secondarily, what was represented in the images. If I describe a sound as high volume, for instance, it’s because it sounded loud relative to the surrounding sounds, but also because there was likely a peak on the waveform at that point. When I struggled to discern features of the sound, I relied more heavily on the waveform and spectrogram. I don’t use numerical units like decibels—although they are measured by Audacity—because those units aren’t reflective of the way in which I used the technology, which wasn’t to isolate “absolute” features of the sound at a given moment in time, but to compare the sound across time. And, regardless, I don’t hear in decibels: using a technology like Audacity does not negate the affective experience of listening, despite the sheen of objectivity afforded by numeric units. Furthermore, using precise units presumes precision that I didn’t have in my listening experience; for instance, I used Bluetooth headphones, which likely compressed the sound as I heard it, so what I saw on Audacity—even by the numbers—is not necessarily representative of what I heard. </p>

<p>One benefit of relative descriptors is that I can use the same terms to describe a variety of sounds; unlike technical vocabularies, they are general enough to be flexible. Another is that, because they’re relative, readers seeking to understand any given description must engage with the surrounding annotations and perhaps also the audiotext itself. For instance, a high volume sound in the Mic 2 recording is louder than the Mic 1 recording, because the Mic 2 recording is generally louder. And, each of the audiotexts have loud sections around 0:40-0:50 and 2:20-2:30, which make the surrounding sounds seem quieter. A term like "high volume" is therefore used inconsistantly by the standards of absolute volume, but consistently with my affective experience of the sound. This centers polyphony in a conceptual sense, as a deeper understanding of the sound comes from encountering multiple sources of sound: my annotations and the audiotexts themselves. Furthermore, it displaces meaning from any single location, written or sounded. This is a descriptive practice which—while lacking in rigor and not suited for all purposes—is consistent with the polyphony, fluidity, and openness of the audiotexts themselves.</p>

## Indexing

<p>In this section, I extend my discussion of reflexivity to the time-based indexing practices of excerpting and timestamping. Here, I am reflexive in my consideration of the contingency of accessing sound through control of its temporal situation. Unconstrained and on-demand access to sound is based on its status as a digital media object that can be saved, played, paused, rewinded, and excerpted. These actions are enabled by technology; I discuss my use of Audacity here, but tech functions in other ways to encode sound as data, to package that data in a transferable and sustainable format, and to playback that data. In some instances, the effects of mediation can be present in a piece of audio. For instance, distortions in a wax cylinder can lead to audible cracks and humming. Wolfgang Ernst calls this “media-archeological information (about the physically real event)” (qtd. in Camlot 2). Although this is not the case for these recordings, I approach indexing in a similar way: it is information not about the event of the sound, but rather the temporality of my own listening event (and the subsequent creation of my annotations). I introduce time-based indexing in my discussion of excerpting, and further apply it to my timestamps, which are layered and variable-length.</p>

<p><i>Excerpting</i></p>

<p>My annotations span an excerpt of 00:06:35 from the full performance, which included several performers and lasted roughly an hour and a half. Avasilichioaei’s original performance was about 16 minutes; the excerpted portion represents the end of her performance (cutting off right before the closing applause). I chose this portion because it is generally representative of the varieties of voices and sounds present throughout the performance. It is long enough to give a sense of the performance, and short enough to allow me to provide detailed annotations.</p>

<p>My use of this excerpt, and the ways in which I approach it, are significant to the situation of each of the recordings in relation to each other and in relation to the annotations. My excerpted section occurs at different start and end points in each of the full-length recordings, owing to different start times. Rather than reflect these times in my timestamps, I timestamp based on the excerpted duration, from 0:00:00 to 00:06:35, for all three recordings. This reflects the remediation of the recordings and facilitates comparison between the annotations.</p>

<p>To begin timestamping at 0:00:00 is to acknowledge a temporality (and set of temporal contingencies) that is distinct to the excerpts and the context in which I place them. This is distinct from the temporality of the live performance, which Friedrich Kittler calls “real time,” (5) and cannot be represented as a duration. To represent a time period as a duration entails a set of technologically mediated, time-based practices like pausing, clipping, and rewinding, which Kittler refers to as “time axis manipulation” (3). Both the original audio clips and my excerpted versions engender not only the possibility of, but the active use of time axis manipulation. The presence of an original and an excerpt implies the use of some technological means of cutting and then saving an excerpted form, which by this process of time axis manipulation is not merely a derivative of the original but a new audiotext in its own right. Thus the excerpt is both predicated on and subject to time axis manipulation in this project. </p>

<p><i>Layered timestamps and indexes</i></p>
<p>At most instances in any of the recordings, there are multiple types of sound audible. These range from an electronic bass tone to voices to static distortion. They become audible and inaudible at different times, and change at varying intervals. In these ways, they exist in time independently from each other. For this reason, I’ve timestamped them independently. This creates a non-chronological temporality in my timestamps, as layers of sound overlap. A sound cannot be said to have completely occurred before the next indexed sound, only to have begun before it. Consider, for instance, this section from Mic 1:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:05:50</td>
    <td>0:06:30</td> 
    <td>Avasilichioaei's voice, speaking with variable speed, low-mid volume and long pauses between phrases. Generally soft onset and offset.</td>
  </tr>
  <tr>
    <td>0:05:59</td>
    <td>0:06:08</td> 
    <td>Mid pitch, mid volume rough cracking friction sound at varying intervals and with varying tones</td>
  </tr>
  <tr>
    <td>0:06:06</td>
    <td>0:06:18</td> 
    <td>Low volume, high pitch fluctuating electronic tone</td>
  </tr>
</table>

<p>One moment in time, such as 0:06:07, corresponds to multiple indices and thus has multiple layers, none of which—in the Mic 1 audiotext—are subordinate to each other even when they are quiet (as in the first entry above), sporadic (as in the second) or short (as in the third). Mic 3 has similar interactions between layers of sounds, although they blend together a bit more, leading to more annotations that group sounds together.</p>

<p>Mic 2 is also polyphonic, but it has clearer audio quality and consequently clearly defined layers of sound. Because the layers of sound are more distinct, I am able to identify layers not just based on time (i.e. through simultaneous timestamps) but also by shared sonic characteristics between sounds at different points in the audiotext. These shared qualities inform my use of descriptive indexes to denote three general categories of sound in the Mic 2 audiotext: speaking voices, electronic tones and distortions, and friction sounds. These labels appear in a fourth column in the annotations, giving readers a way to parse sounds based on timbre, not time. Speaking voices includes any vocalizations: live, recorded, or otherwise modulated. Electronic tones and distortions include any sounds with buzzy and/or synthetic timbre. Friction sounds refers to friction in the literal sense (distinct from the conceptual friction I've referred to). These are percussive sounds that seem to be produced by the motion of two objects together (on a macro scale). The latter two categories are represented in this sample:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:36</td>
    <td>0:02:04</td> 
    <td>Gravelley, crunching sound beginning with a period of ~0.5 second. Low to mid pitch and volume. Period becomes longer and volume decreases through this interval.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:01:41</td>
    <td>0:02:02</td> 
    <td>Clear scratch sound with varying pitch and generally low volume, like a wooden stick tracing across a wooden table.</td>
    <td>Friction sounds</td>
  </tr>
  <tr>
    <td>0:02:10</td>
    <td>0:02:12</td> 
    <td>Polyphonic electronic tone crescendos to high volume</td>
    <td>Electronic tones and distortions</td>
  </tr>
</table>

<p>By attending to the polyphonic nature of each of these audiotexts, and the distinct manner in which sounds stratify, blend, and remediate each other, I attempt to reflect the variances in sonic layering in each.</p>

<p><i>Variable-length timestamps</i></p>

<p>To the extent that my annotations are also a visual (i.e. written text) interpretation of sound, I’ve used Audacity to preserve what may now be called “visual noise.” All of these audiotexts are noisy, in the sense that they include uncontrolled, unauthorized, and (especially in Mic 1 and Mic 3) spatially resonant sound. Often, technologies like Audacity are used to isolate sounds like these; the presentation of sound as visualized data facilitates a listening/viewing practice informed by control of the (desired) data set, which includes the designation of outliers as noise and their subsequent removal.</p>
<p>Timestamps can also be used to delimit between "meaningful" sound, which is catalogued, and insignificant sound, which is not. At some level, it is necessary to pick and choose how sounds will be grouped together, what will be registered, and what will not when attempting to create annotations. A timestamp determines the boundaries of a specific sonic event, gives it visibility in a visual realm, informs the listening experience, and places it in conversation with surrounding sounds. Rather than making noise invisible, timestamping can therefore be a tool to assert its presence. </p>
<p>I timestamp noise to avoid drawing an arbitrary boundary between what the sound "should be" and what it actually is. In fact, timestamping noise is helpful in encouraging reflexivity, because it puts constructive pressure on other ways of annotating and/or transcribing sound that exclude it without explanation. Because noise is a social construct, what's noisy differs between situations and between people. Rather than imposing my definition of noise (or the definition that informs the development of audio editing software) readers of reflexive annotation may decide for themselves where the noise is, possibly reassessing their own concept of noise.</p>
<p>As with all of the sounds I timestamp, I render noise differently depending on how it occupies time. I group noisy sounds together when they are in proximity, as in the following from Mic 3:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:00:07</td>
    <td>0:00:08</td> 
    <td>Series of friction sounds (pops, cracks, gravelley sounds) in quick succession, at mid volume and mid-high pitch</td>
  </tr>
</table>

<p>Other times, I timestamp them individually, as in this annotation from Mic 1:</p>

<table>
  <tr>
    <th>Start time</th>
    <th>End time</th> 
    <th>Annotation</th>
  </tr>
  <tr>
    <td>0:01:02</td>
    <td> </td> 
    <td>Pop with hard onset and soft offset</td>
  </tr>
</table>

<p>As above, all of the annotations under one second have only an instantaneous timestamp. A timestamp under one second is a bit ridiculous in terms of what’s useful or typical, but it’s helpful in making evident my use of Audacity, as use of technology is clearly required to document sounds this short. Furthermore, annotations with short timestamps take up the same amount of visual space as annotations with longer timestamps, a lack of differentiation that is more true to the audiotexts, where noise is not cordoned off or muted. </p>

<p>Somewhat unexpectedly, there are more “visual noise” timestamps in my Mic 1 annotations than either of the other annotations, although its spectrogram in Audacity appears less noisy than Mic 3. This is because the noise in Mic 1 is mostly short pops and cracks which are not consistant with the surrounding sounds. In Mic 3, the noise seems to blend together, which is likely not a function of the noise being drastically different between the two audiotexts, but rather of the busier character of the Mic 3 audiotext. Where do the differences in the noise come from? It could be differences in the recording devices, or their different placements in the room. Ultimately, they come from contextual histories that I don't gain access to through annotation. Therefore, I do not attempt to impose a context onto the noise in the audiotexts.</p>

## Paratext

<p>In this section, I discuss paratext of annotation. I use Genette’s definition of paratext as that which constitutes the “threshold” of a text, which through its framing of the text leads readers to “a better reception of the text and a more pertinent reading—more pertinent, naturally, in the eyes of the author” (261, 262). In this essay, itself a paratext of my annotations, I have attempted to frame reflexive annotation as a process of responsible creation of artifacts of a situated listening experience. This is the framing I hope to reassert in all of the paratext of this project (or at least the pieces of paratext that I can control). This section is less focused on the "Operator" audiotexts than the previous two; rather, I shift my focus outward, to reflexively considering the presentation and reception of this project as a whole.</p>

<p>I begin by asking what and where the paratext is in this project. Differentiating text from paratext is not a trivial task; Genette writes, “The ways and means of the paratext are modified unceasingly according to periods, cultures, genres, authors, works, [and] editions of the same work” (262). Audio annotation itself can be a paratext in, for instance, digital collections of archival audio, where indexed annotations serve as a table of contents to the main (audio)text. This frames annotation as a usable derivative of the main audiotext, to be received without critical consideration of its contents. </p>

<p>Reflexive annotation is based on understanding annotation as subjective, interpretive, and singular. In this instance, the audio is a paratext to the annotations. One indicator of this is my use of AVAnnotate, the annotation-centered platform I used to create this project. In this section, I introduce AVAnnotate's branding and affordances as an example of epitext that legitimizes annotation as an object of study. I then discuss the formatting of the annotations relative to the audio.</p>

<p><i>AVAnnotate</i></p>

<p>AVAnnotate formats user-created annotations of audio and/or video, and generates GitHub Pages websites to host these annotations, along with contextual writing and/or images. Because each AVAnnotate project has its own, independently-accessible website, the AVAnnotate website and branding belongs to a subcategory of paratext that Genette calls <i>epitext</i>. Epitext is material that is closely associated with the text, but outside the space of the volume (the book, or in this case this website) itself (264). Although this website can be accessed independently from the AVAnnotate website, the platform provides much of the framing for this project. AVAnnotate is oriented primarily towards researchers and scholars who want to create (not just view) annotations. Framing annotation as an end in itself highlights the critical-analytical and/or reflective possibilities engendered in the act of creating annotations. 
Consequently, AVAnnotate allows users lots of latitude in determining layout, appearance, and contextualizing information; while the AVAnnotate platform itself is used to upload indexed annotations, the project website can be edited in any number of ways via its linked GitHub repository. It’s a tool that doesn’t presume an end function, and as such it reorients (creative and receptive) focus to the process of creating and contextualizing annotations.</p>

<p><i>Formatting of annotation pages</i></p>

<p>Following from the affordances of AVAnnotate, I utilize <i>peritext</i>—paratext inside this website—to highlight the independence of my annotations from the audio itself (Genette 263). Peritext manifests in many ways; for instance, the title of this website, and the descriptions in the footer and at the beginning of the pages are all examples of peritext. In these instances, many of my choices are self-evident. For instance, I place the term “reflexive annotation” early in my title and the footer description to emphasize that this is a project about critical uses of annotation, not merely a host for audio. To avoid repeating the self-evident, I do not discuss every instance of peritext here but focus on the formatting of the annotation pages, via the viewer selection.</p>

<p>AVAnnotate supports two IIIF-compatible viewers: the Aviary Player and the Universal Viewer. The Aviary Player format integrates the video/audio playback and the annotations into a single view, with the playback occupying the left and center two-thirds of the view, and the annotations at the right third of the screen. The Universal Viewer has more separation between the playback and the annotations, where the playback is at the top of the screen and the annotations follow. I use the Universal Viewer because it’s more practical for an audio-only project, but also because it visually separates the audio from the annotations. This separation supports the principle of responsibility to sounded alterity that I’ve used throughout this project, because it highlights that my annotations do not enter into the space or time of the audio itself, and rather serves as an exterior artifact of a mediated listening experience.</p>

## Conclusion

<p>This project introduction explores how the central axioms of an audio annotation process can be determined not by usage conventions, but instead by reflexivity in the annotation process. I define reflexively as responsibility to the network of relations that (re)constitute the audiotext. This encompasses the location of sociopolitical values and aims within my own listening experience and respect for the alterity of the audiotext.
</p>

<p>I apply reflexivity in three areas of my annotations: description, indexing, and paratext. I do this by tracing the encounters I engage in, with the sound of the audiotext but also with vocabulary schemes for describing sound, with Audacity, and with AVAnnotate. The resulting annotations utilize relative language that highlights semiotic and polyphonic features of the audiotexts; layered timestamps that reflect the temporal mediation of excerpting and inscribing non-chronological sonic events; and paratext that foregrounds the non-derivative, situated subjectivity of each annotation set. My annotations are not primarily about the sounds I listened to; they’re a document of the processes I undertook to attempt to inscribe those sounds in the most reflexive way possible. Ultimately, and perhaps paradoxically, I found that to annotate reflexively means to give up the goal of accessing sound through written language. I found myself more drawn to the question of why even annotate? What can annotation do when it isn’t taken for granted? Reflexive annotation gives me three answers to these questions.</p>

<p>	<b>1. Reflexive annotation provides a (preliminary) framework for critically and responsibly working with literary audio.</b>I began this essay with a discussion of Unsworth’s scholarly primitives, which are the guiding processes by which knowledge creation is approached in institutional, academic 	settings. These can be taken for granted with respect to print texts; I don’t remember any particular moment when I decided how I would write marginal notes in printed books, or which criteria I would use to pick out quotes from a text. I rarely begin with a methodology in mind, even through my intuitive practices are just as contingent and situated in specific sociopolitical and epistemological aims as the audio-related practices I’ve discussed here. Working with audio is a less intuitive practice not because of some inherent incompatibility, but because of the friction between the print-related practices I know and the reality that digital audio is a completely different form of media.</p>
	
<p>	Attempting to work through this gap, while frequently frustrating, is also the locus for a methodological approach to sound that is attuned to its own status as methodology. To meet that friction, and to consider what it sounds like and where it comes from, is an interesting and (as I’ve tried to demonstrate in this essay) generative prospect. From this lens, I can build a framework for writing about, indexing, and publishing work about 	audio that is less constrained by the norms of print.</p>

<p>	<b>2. Reflexive annotation documents a listening practice.</b> Annotation doesn’t preserve the sensory experience of listening, but it does preserve the representational choices produced by/within a specific way of listening. These choices can then be analyzed, reconsidered, and responded to. One audiotext can be annotated in multiple different ways, and the ways in which it is reconstituted can be compared. Through reflexive annotation, listening is not a ephemeral, passive process of obtaining information but itself a product of its own situation, which is reproduced in annotation </p>

<p>	<b>3. Reflexive annotation is decentralized.</b> It takes into consideration both processes that aren’t visible in the annotations (e.g. my use of Audacity) and elements that are visible, but outside of the space of the annotation itself (e.g. paratexts via AVAnnotate). Because reflexive annotation is decentralized, it brings the technologies I use under the umbrella of critical consideration. Both my process and the resulting annotations would have varied greatly if I used different tools, particularly in the case of AVAnnotate. As far as I’m aware, there isn’t another platform that is oriented specifically towards the creation of annotations for digital audio. It would have been much more difficult to create this project without the use of AVAnnotate. Because reflexive annotation is linked to the affordances of the platform(s) it uses, it highlights the importance of platforms—like AVAnnotate—that provide latitude for practical applications of reflexivity.</p>


## Works Cited

<p>Bernstein, Charles. “Introduction.” <i>Close Listening: Poetry and the Performed Word</i>, Oxford University Press, New York, NY, 1998, pp. 3–26.</p>

<p>Camlot, Jason, and Christine Mitchell. “Amodern 4: The Poetry Series.” <i>Amodern</i>, vol. 4, Mar. 2015, https://doi.org/https://amodern.net/issues/amodern-4/.</p>

<p>Camlot, Jason. <i>Phonopoetics: The Making of Early Literary Recordings</i>. Stanford University Press, 2019.</p>

<p>Chion, Michel. “The Three Listening Modes.” <i>Audio-Vision: Sound on Screen</i>, translated by Claudia Gorbman, Second ed., Columbia University Press, New York, NY, 2019, pp. 22–34.</p>

<p>Clement, Tanya E., and Liz Fischer. “Audiated Annotation from the Middle Ages to the Open Web.” <i>Digital Humanities Quarterly</i>, vol. 15, no. 1, 2021, http://www.digitalhumanities.org/dhq/vol/15/1/000512/000512.html.</p>

<p>​​“Collections.” <i>Aviary</i>, University of Alberta, ualberta.aviaryplatform.com/collection. Accessed 15 Apr. 2024. </p>

<p>Genette, Gérard. “Introduction to the Paratext.” Translated by Marie Maclean. <i>New Literary History</i>, vol. 22, no. 2, spring 1991, pp. 261–272, https://doi.org/10.2307/469037.</p>

<p>Kittler, Friedrich. “Real Time Analysis, Time Axis Manipulation.” Translated by Geoffrey Winthrop-Young. <i>Cultural Politics</i>, vol. 13, no. 1, 2017, pp. 1–18, https://doi.org/10.1215/17432197-3755144.</p>

<p>Lipari, Lisbeth. <i>Listening, Thinking, Being: Toward an Ethics of Attunement</i>. The Pennsylvania State University Press, 2014.

<p>McGann, Jerome J. <i>The Textual Condition</i>. Princeton University Press, 1991.</p>

<p>Robinson, Dylan. <i>Hungry Listening: Resonant Theory for Indigenous Sound Studies</i>. University of Minnesota Press, 2021.</p>

<p>Sterne, Jonathan. “Hearing.” <i>Keywords in Sound</i>, Duke University Press, 2015, pp. 65–77.</p>

<p>Unsworth, John. “Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This?” John M. Unsworth: Conference Papers and Presentations, University of Virginia, 13 May 2000, people.brandeis.edu/~unsworth/Kings.5-00/primitives.html.</p>
